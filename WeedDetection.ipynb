{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WeedDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMAswer5WU4apOWvxW85SrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikrithik/Weed-Detection/blob/main/WeedDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ys-zQd2KBs1",
        "outputId": "42335c55-1e4c-4ae3-9a18-03bd56c424f8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Mar 13 07:37:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    29W /  70W |   9024MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdyF3hwxLFZs",
        "outputId": "abaaca49-18dd-45a7-d2c5-e9dbc8c57c4d"
      },
      "source": [
        "!git clone https://github.com/AlexOlsen/DeepWeeds.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DeepWeeds' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpXCsVhmJhTo"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "from time import perf_counter \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPool2D,BatchNormalization\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\r\n",
        "from urllib.request import urlopen\r\n",
        "import shutil\r\n",
        "import pandas as pd\r\n",
        "from time import time\r\n",
        "from datetime import datetime\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\r\n",
        "from keras.optimizers import Adam\r\n",
        "import csv\r\n",
        "from keras.models import Model, load_model\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report\r\n",
        "from keras import backend as K\r\n",
        "from skimage.io import imread\r\n",
        "from skimage.transform import resize\r\n",
        "from keras.applications.inception_v3 import InceptionV3\r\n",
        "from keras.applications.resnet50 import ResNet50\r\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\r\n",
        "\r\n",
        "\r\n",
        "import pickle\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "from zipfile import ZipFile\r\n",
        "from urllib.request import urlopen\r\n",
        "import shutil\r\n",
        "import pandas as pd\r\n",
        "from time import time\r\n",
        "from datetime import datetime\r\n",
        "import requests\r\n",
        "\r\n",
        "\r\n",
        "# Global paths\r\n",
        "OUTPUT_DIRECTORY = \"./outputs/\"\r\n",
        "LABEL_DIRECTORY = \"/content/DeepWeeds/labels/\"\r\n",
        "MODEL_DIRECTORY = \"./models/\"\r\n",
        "MODEL_GD_ID = \"1MRbN5hXOTYnw7-71K-2vjY01uJ9GkQM5\"\r\n",
        "MODEL_ZIP_FILE = \"./models/models.zip\"\r\n",
        "IMG_DIRECTORY = \"./images\"\r\n",
        "IMG_GD_ID = \"1xnK3B6K6KekDI55vwJ0vnc2IGoDga9cj\"\r\n",
        "IMG_ZIP_FILE = \"./images/images.zip\"\r\n",
        "\r\n",
        "# Global variables\r\n",
        "RAW_IMG_SIZE = (256, 256)\r\n",
        "IMG_SIZE = (224, 224)\r\n",
        "INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)\r\n",
        "MAX_EPOCH = 15\r\n",
        "BATCH_SIZE = 64\r\n",
        "FOLDS = 5\r\n",
        "STOPPING_PATIENCE = 5\r\n",
        "LR_PATIENCE = 5\r\n",
        "INITIAL_LR = 0.0001\r\n",
        "CLASSES = ['0', '1', '2', '3', '4', '5', '6', '7', '8']\r\n",
        "CLASS_NAMES = ['Chinee Apple',\r\n",
        "               'Lantana',\r\n",
        "               'Parkinsonia',\r\n",
        "               'Parthenium',\r\n",
        "               'Prickly Acacia',\r\n",
        "               'Rubber Vine',\r\n",
        "               'Siam Weed',\r\n",
        "               'Snake Weed',\r\n",
        "               'Negatives']\r\n",
        "class_dict = {0:'Chinee Apple',\r\n",
        "               1:'Lantana',\r\n",
        "               2:'Parkinsonia',\r\n",
        "               3:'Parthenium',\r\n",
        "               4:'Prickly Acacia',\r\n",
        "               5:'Rubber Vine',\r\n",
        "               6:'Siam Weed',\r\n",
        "               7:'Snake Weed',\r\n",
        "               8:'Negatives'\r\n",
        "}\r\n",
        "def download_google_drive_file(id, destination):\r\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\r\n",
        "\r\n",
        "    session = requests.Session()\r\n",
        "\r\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\r\n",
        "    token = get_confirm_token(response)\r\n",
        "\r\n",
        "    if token:\r\n",
        "        params = { 'id' : id, 'confirm' : token }\r\n",
        "        response = session.get(URL, params = params, stream = True)\r\n",
        "\r\n",
        "    save_response_content(response, destination)\r\n",
        "def get_confirm_token(response):\r\n",
        "    for key, value in response.cookies.items():\r\n",
        "        if key.startswith('download_warning'):\r\n",
        "            return value\r\n",
        "\r\n",
        "    return None\r\n",
        "\r\n",
        "\r\n",
        "def save_response_content(response, destination):\r\n",
        "    CHUNK_SIZE = 32768\r\n",
        "\r\n",
        "    with open(destination, \"wb\") as f:\r\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\r\n",
        "            if chunk: # filter out keep-alive new chunks\r\n",
        "                f.write(chunk)\r\n",
        "\r\n",
        "\r\n",
        "def parse_args():\r\n",
        "    parser = argparse.ArgumentParser(description='Train and test ResNet50, InceptionV3, or custom model on DeepWeeds.')\r\n",
        "    parser.add_argument(\"command\", default='train', help=\"'cross_validate' or 'inference'\")\r\n",
        "    parser.add_argument('--model', default='resnet', help=\"'resnet', 'inception', or path to .hdf5 file.\")\r\n",
        "    args = parser.parse_args()\r\n",
        "    return args.command, args.model\r\n",
        "\r\n",
        "\r\n",
        "def download_images():\r\n",
        "    if not os.path.exists(IMG_DIRECTORY):\r\n",
        "        os.makedirs(IMG_DIRECTORY)\r\n",
        "        print(\"Downloading DeepWeeds images to \" + IMG_ZIP_FILE)\r\n",
        "        download_google_drive_file(IMG_GD_ID, IMG_ZIP_FILE)\r\n",
        "        print(\"Finished downloading images.\")\r\n",
        "        print(\"Unzipping \" + IMG_ZIP_FILE)\r\n",
        "        with ZipFile(IMG_ZIP_FILE, \"r\") as zip_ref:\r\n",
        "            zip_ref.extractall(IMG_DIRECTORY)\r\n",
        "        print(\"Finished unzipping images.\")\r\n",
        "\r\n",
        "\r\n",
        "def download_models():\r\n",
        "    if not os.path.exists(MODEL_DIRECTORY):\r\n",
        "        os.makedirs(MODEL_DIRECTORY)\r\n",
        "        print(\"Downloading DeepWeeds models to \" + MODEL_ZIP_FILE)\r\n",
        "        download_google_drive_file(MODEL_GD_ID, MODEL_ZIP_FILE)\r\n",
        "        print(\"Finished downloading models.\")\r\n",
        "        print(\"Unzipping \" + MODEL_ZIP_FILE)\r\n",
        "        with ZipFile(MODEL_ZIP_FILE, \"r\") as zip_ref:\r\n",
        "            zip_ref.extractall(MODEL_DIRECTORY)\r\n",
        "        print(\"Finished unzipping models.\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DFmia1sKae7"
      },
      "source": [
        "download_images()\r\n",
        "download_models()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZzfqYbJMKkk",
        "outputId": "dfea8e9b-ae59-4dfc-cc7b-773dd86cf693"
      },
      "source": [
        "img_width, img_height = 224, 224\r\n",
        "\r\n",
        "\r\n",
        "# Define VGG16\r\n",
        "classifier=Sequential()\r\n",
        "\r\n",
        "classifier.add(Conv2D(filters=96,kernel_size=(11,11),strides=(4,4),padding='valid',input_shape=(img_width,img_height,3),activation='relu'))\r\n",
        "classifier.add(MaxPool2D((2,2),strides=(2,2),padding='valid'))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "\r\n",
        "classifier.add(Conv2D(filters=128,kernel_size=(5,5),strides=(1,1),padding='valid',activation='relu'))\r\n",
        "classifier.add(MaxPool2D((2,2),strides=(2,2),padding='valid'))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "\r\n",
        "classifier.add(Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='valid',activation='relu'))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "\r\n",
        "classifier.add(Conv2D(filters=512,kernel_size=(3,3),strides=(1,1),padding='valid',activation='relu'))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "\r\n",
        "classifier.add(Conv2D(filters=1024,kernel_size=(3,3),strides=(1,1),padding='valid',activation='relu'))\r\n",
        "\r\n",
        "classifier.add(MaxPool2D((2,2),strides=(2,2),padding='valid'))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "\r\n",
        "classifier.add(Flatten())\r\n",
        "\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "classifier.add(Dense(1024,activation='relu'))\r\n",
        "classifier.add(Dropout(0.4))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "classifier.add(Dense(512,activation='relu'))\r\n",
        "classifier.add(Dropout(0.2))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "classifier.add(Dense(9,activation='sigmoid'))\r\n",
        "\r\n",
        "classifier.summary()\r\n",
        "# classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 54, 54, 96)        34944     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 27, 27, 96)        384       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 23, 23, 128)       307328    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 11, 11, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 9, 9, 512)         590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 9, 9, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 5, 5, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 9)                 4617      \n",
            "=================================================================\n",
            "Total params: 12,768,393\n",
            "Trainable params: 12,752,585\n",
            "Non-trainable params: 15,808\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99CeQMRTK9mc"
      },
      "source": [
        "def crop(img, size):\r\n",
        "    \"\"\"\r\n",
        "    Crop the image concentrically to the desired size.\r\n",
        "    :param img: Input image\r\n",
        "    :param size: Required crop image size\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    (h, w, c) = img.shape\r\n",
        "    x = int((w - size[0]) / 2)\r\n",
        "    y = int((h - size[1]) / 2)\r\n",
        "    return img[y:(y + size[1]), x:(x + size[0]), :]\r\n",
        "\r\n",
        "\r\n",
        "def crop_generator(batches, size):\r\n",
        "    \"\"\"\r\n",
        "    Take as input a Keras ImageGen (Iterator) and generate random\r\n",
        "    crops from the image batches generated by the original iterator\r\n",
        "    :param batches: Batches of images to be cropped\r\n",
        "    :param size: Size to be cropped to\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    while True:\r\n",
        "        batch_x, batch_y = next(batches)\r\n",
        "        (b, h, w, c) = batch_x.shape\r\n",
        "        batch_crops = np.zeros((b, size[0], size[1], c))\r\n",
        "        for i in range(b):\r\n",
        "            batch_crops[i] = crop(batch_x[i], (size[0], size[1]))\r\n",
        "        yield (batch_crops, batch_y)\r\n",
        "\r\n",
        "\r\n",
        "def cross_validate():\r\n",
        "\r\n",
        "    # K fold cross validation, saving outputs for each fold\r\n",
        "    for k in range(FOLDS):\r\n",
        "\r\n",
        "        # Create new output directory for individual folds from timestamp\r\n",
        "        timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\r\n",
        "        print('Fold {}/{} - {}'.format(k + 1, FOLDS, timestamp))\r\n",
        "        output_directory = \"{}{}/\".format(OUTPUT_DIRECTORY, timestamp)\r\n",
        "        if not os.path.exists(output_directory):\r\n",
        "            os.makedirs(output_directory)\r\n",
        "\r\n",
        "        # Prepare training, validation and testing labels for kth fold\r\n",
        "        train_label_file = \"{}train_subset{}.csv\".format(LABEL_DIRECTORY, k)\r\n",
        "        val_label_file = \"{}val_subset{}.csv\".format(LABEL_DIRECTORY, k)\r\n",
        "        test_label_file = \"{}test_subset{}.csv\".format(LABEL_DIRECTORY, k)\r\n",
        "        train_dataframe = pd.read_csv(train_label_file).astype(str)\r\n",
        "        val_dataframe = pd.read_csv(val_label_file).astype(str)\r\n",
        "        test_dataframe = pd.read_csv(test_label_file).astype(str)\r\n",
        "        train_image_count = train_dataframe.shape[0]\r\n",
        "        val_image_count = train_dataframe.shape[0]\r\n",
        "        test_image_count = test_dataframe.shape[0]\r\n",
        "\r\n",
        "        # Training image augmentation\r\n",
        "        train_data_generator = ImageDataGenerator(\r\n",
        "            rescale=1. / 255,\r\n",
        "            fill_mode=\"constant\",\r\n",
        "            shear_range=0.2,\r\n",
        "            zoom_range=(0.5, 1),\r\n",
        "            horizontal_flip=True,\r\n",
        "            rotation_range=360,\r\n",
        "            channel_shift_range=25,\r\n",
        "            brightness_range=(0.75, 1.25))\r\n",
        "\r\n",
        "        # Validation image augmentation\r\n",
        "        val_data_generator = ImageDataGenerator(\r\n",
        "            rescale=1. / 255,\r\n",
        "            fill_mode=\"constant\",\r\n",
        "            shear_range=0.2,\r\n",
        "            zoom_range=(0.5, 1),\r\n",
        "            horizontal_flip=True,\r\n",
        "            rotation_range=360,\r\n",
        "            channel_shift_range=25,\r\n",
        "            brightness_range=(0.75, 1.25))\r\n",
        "\r\n",
        "        # No testing image augmentation (except for converting pixel values to floats)\r\n",
        "        test_data_generator = ImageDataGenerator(rescale=1. / 255)\r\n",
        "        # Load train images in batches from directory and apply augmentations\r\n",
        "        train_data_generator = train_data_generator.flow_from_dataframe(\r\n",
        "            train_dataframe,\r\n",
        "            IMG_DIRECTORY,\r\n",
        "            x_col='Filename',\r\n",
        "            y_col='Label',\r\n",
        "            target_size=RAW_IMG_SIZE,\r\n",
        "            batch_size=BATCH_SIZE,\r\n",
        "            classes=CLASSES,\r\n",
        "            class_mode='categorical')\r\n",
        "\r\n",
        "        # Load validation images in batches from directory and apply rescaling\r\n",
        "        val_data_generator = val_data_generator.flow_from_dataframe(\r\n",
        "            val_dataframe,\r\n",
        "            IMG_DIRECTORY,\r\n",
        "            x_col=\"Filename\",\r\n",
        "            y_col=\"Label\",\r\n",
        "            target_size=RAW_IMG_SIZE,\r\n",
        "            batch_size=BATCH_SIZE,\r\n",
        "            classes=CLASSES,\r\n",
        "            class_mode='categorical')\r\n",
        "\r\n",
        "        # Load test images in batches from directory and apply rescaling\r\n",
        "        test_data_generator = test_data_generator.flow_from_dataframe(\r\n",
        "            test_dataframe,\r\n",
        "            IMG_DIRECTORY,\r\n",
        "            x_col=\"Filename\",\r\n",
        "            y_col=\"Label\",\r\n",
        "            target_size=IMG_SIZE,\r\n",
        "            batch_size=BATCH_SIZE,\r\n",
        "            shuffle=False,\r\n",
        "            classes=CLASSES,\r\n",
        "            class_mode='categorical')\r\n",
        "\r\n",
        "        # Crop augmented images from 256x256 to 224x224\r\n",
        "        train_data_generator = crop_generator(train_data_generator, IMG_SIZE)\r\n",
        "        val_data_generator = crop_generator(val_data_generator, IMG_SIZE)\r\n",
        "\r\n",
        "        # Load ImageNet pre-trained model with no top, either InceptionV3 or ResNet50\r\n",
        "        # if model_name == \"resnet\":\r\n",
        "        #     base_model = ResNet50(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\r\n",
        "        # elif model_name == \"inception\":\r\n",
        "        #     base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\r\n",
        "        # x = base_model.output\r\n",
        "        # # Add a global average pooling layer\r\n",
        "        # x = GlobalAveragePooling2D(name='avg_pool')(x)\r\n",
        "        # # Add fully connected output layer with sigmoid activation for multi label classification\r\n",
        "        # outputs = Dense(len(CLASSES), activation='sigmoid', name='fc9')(x)\r\n",
        "        # Assemble the modified model\r\n",
        "        model =  classifier#Model(inputs=base_model.input, outputs=outputs)\r\n",
        "\r\n",
        "        # Checkpoints for training\r\n",
        "        model_checkpoint = ModelCheckpoint(output_directory + \"lastbest-0.hdf5\", verbose=1, save_best_only=True)\r\n",
        "        early_stopping = EarlyStopping(patience=STOPPING_PATIENCE, restore_best_weights=True)\r\n",
        "        tensorboard = TensorBoard(log_dir=output_directory, histogram_freq=0, write_graph=True, write_images=False)\r\n",
        "        reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=LR_PATIENCE)\r\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=INITIAL_LR), metrics=['categorical_accuracy'])\r\n",
        "        csv_logger = CSVLogger(output_directory + \"training_metrics.csv\")\r\n",
        "\r\n",
        "        # Train model until MAX_EPOCH, restarting after each early stop when learning has plateaued\r\n",
        "        global_epoch = 0\r\n",
        "        restarts = 0\r\n",
        "        last_best_losses = []\r\n",
        "        last_best_epochs = []\r\n",
        "        while global_epoch < MAX_EPOCH:\r\n",
        "            history = model.fit_generator(\r\n",
        "                generator=train_data_generator,\r\n",
        "                steps_per_epoch=train_image_count // BATCH_SIZE,\r\n",
        "                epochs=MAX_EPOCH - global_epoch,\r\n",
        "                validation_data=val_data_generator,\r\n",
        "                validation_steps=val_image_count // BATCH_SIZE,\r\n",
        "                callbacks=[tensorboard, model_checkpoint, early_stopping, reduce_lr, csv_logger],\r\n",
        "                shuffle=False)\r\n",
        "            last_best_losses.append(min(history.history['val_loss']))\r\n",
        "            last_best_local_epoch = history.history['val_loss'].index(min(history.history['val_loss']))\r\n",
        "            last_best_epochs.append(global_epoch + last_best_local_epoch)\r\n",
        "            if early_stopping.stopped_epoch == 0:\r\n",
        "                print(\"Completed training after {} epochs.\".format(MAX_EPOCH))\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                global_epoch = global_epoch + early_stopping.stopped_epoch - STOPPING_PATIENCE + 1\r\n",
        "                print(\"Early stopping triggered after local epoch {} (global epoch {}).\".format(\r\n",
        "                    early_stopping.stopped_epoch, global_epoch))\r\n",
        "                print(\"Restarting from last best val_loss at local epoch {} (global epoch {}).\".format(\r\n",
        "                    early_stopping.stopped_epoch - STOPPING_PATIENCE, global_epoch - STOPPING_PATIENCE))\r\n",
        "                restarts = restarts + 1\r\n",
        "                model.compile(loss='binary_crossentropy', optimizer=Adam(lr=INITIAL_LR / 2 ** restarts),\r\n",
        "                              metrics=['categorical_accuracy'])\r\n",
        "                model_checkpoint = ModelCheckpoint(output_directory + \"lastbest-{}.hdf5\".format(restarts),\r\n",
        "                                                   monitor='val_loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "\r\n",
        "        # Save last best model info\r\n",
        "        with open(output_directory + \"last_best_models.csv\", 'w', newline='') as file:\r\n",
        "            writer = csv.writer(file, delimiter=',')\r\n",
        "            writer.writerow(['Model file', 'Global epoch', 'Validation loss'])\r\n",
        "            for i in range(restarts + 1):\r\n",
        "                writer.writerow([\"lastbest-{}.hdf5\".format(i), last_best_epochs[i], last_best_losses[i]])\r\n",
        "\r\n",
        "        # Load the last best model\r\n",
        "        model = load_model(\r\n",
        "            output_directory + \"lastbest-{}.hdf5\".format(last_best_losses.index(min(last_best_losses))))\r\n",
        "\r\n",
        "        # Evaluate model on test subset for kth fold\r\n",
        "        predictions = model.predict_generator(test_data_generator, test_image_count // BATCH_SIZE + 1)\r\n",
        "        y_true = test_data_generator.classes\r\n",
        "        y_pred = np.argmax(predictions, axis=1)\r\n",
        "        y_pred[np.max(predictions, axis=1) < 1 / 9] = 8  # Assign predictions worse than random guess to negative class\r\n",
        "\r\n",
        "        # Generate and print classification metrics and confusion matrix\r\n",
        "        print(classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES))\r\n",
        "        report = classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES, output_dict=True)\r\n",
        "        with open(output_directory + 'classification_report.csv', 'w') as f:\r\n",
        "            for key in report.keys():\r\n",
        "                f.write(\"%s,%s\\n\" % (key, report[key]))\r\n",
        "        conf_arr = confusion_matrix(y_true, y_pred, labels=CLASSES)\r\n",
        "        print(conf_arr)\r\n",
        "        np.savetxt(output_directory + \"confusion_matrix.csv\", conf_arr, delimiter=\",\")\r\n",
        "\r\n",
        "        # Clear model from GPU after each iteration\r\n",
        "        print(\"Finished testing fold {}\\n\".format(k + 1))\r\n",
        "        K.clear_session()\r\n",
        "        k = k + 1\r\n",
        "\r\n",
        "\r\n",
        "def inference(model):\r\n",
        "\r\n",
        "    # Create new output directory for saving inference times\r\n",
        "    timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\r\n",
        "    output_directory = \"{}{}/\".format(OUTPUT_DIRECTORY, timestamp)\r\n",
        "    if not os.path.exists(output_directory):\r\n",
        "        os.makedirs(output_directory)\r\n",
        "\r\n",
        "    # Load DeepWeeds dataframe\r\n",
        "    dataframe = pd.read_csv(LABEL_DIRECTORY + \"labels.csv\")\r\n",
        "    image_count = dataframe.shape[0]\r\n",
        "    filenames = dataframe.Filename\r\n",
        "\r\n",
        "    preprocessing_times = []\r\n",
        "    inference_times = []\r\n",
        "    for i in range(image_count):\r\n",
        "        # Load image\r\n",
        "        start_time = time()\r\n",
        "        img = imread(IMG_DIRECTORY + filenames[i])\r\n",
        "        # Resize to 224x224\r\n",
        "        img = resize(img, (224, 224))\r\n",
        "        # Map to batch\r\n",
        "        img = np.expand_dims(img, axis=0)\r\n",
        "        # Scale from int to float\r\n",
        "        img = img * 1./255\r\n",
        "        preprocessing_time = time() - start_time\r\n",
        "        start_time = time()\r\n",
        "        # Predict label\r\n",
        "        prediction = model.predict(img, batch_size=1, verbose=0)\r\n",
        "        y_pred = np.argmax(prediction, axis=1)\r\n",
        "        y_pred[np.max(prediction, axis=1) < 1/9] = 8\r\n",
        "        inference_time = time() - start_time\r\n",
        "        # Append times to lists\r\n",
        "        preprocessing_times.append(preprocessing_time)\r\n",
        "        inference_times.append(inference_time)\r\n",
        "\r\n",
        "    # Save inference times to csv\r\n",
        "    with open(output_directory + \"tf_inference_times.csv\", 'w', newline='') as file:\r\n",
        "        writer = csv.writer(file, delimiter=',')\r\n",
        "        writer.writerow(['Filename', 'Preprocessing time (ms)', 'Inference time (ms)'])\r\n",
        "        for i in range(image_count):\r\n",
        "            writer.writerow([filenames[i], preprocessing_times[i] * 1000, inference_times[i] * 1000])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZfaMA65_JsY0",
        "outputId": "7af1ed04-c03a-4437-c7ce-2945ce22fde4"
      },
      "source": [
        "cross_validate()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1/5 - 20210313-073703\n",
            "Found 10501 validated image filenames belonging to 9 classes.\n",
            "Found 3501 validated image filenames belonging to 9 classes.\n",
            "Found 3507 validated image filenames belonging to 9 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "164/164 [==============================] - 336s 2s/step - loss: 0.8161 - categorical_accuracy: 0.1974 - val_loss: 0.6034 - val_categorical_accuracy: 0.5202\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.60344, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 2/15\n",
            "164/164 [==============================] - 334s 2s/step - loss: 0.6904 - categorical_accuracy: 0.2841 - val_loss: 0.5697 - val_categorical_accuracy: 0.5197\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.60344 to 0.56973, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 3/15\n",
            "164/164 [==============================] - 332s 2s/step - loss: 0.5974 - categorical_accuracy: 0.3396 - val_loss: 0.4674 - val_categorical_accuracy: 0.5157\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.56973 to 0.46743, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 4/15\n",
            "164/164 [==============================] - 331s 2s/step - loss: 0.4968 - categorical_accuracy: 0.4356 - val_loss: 0.3767 - val_categorical_accuracy: 0.5064\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.46743 to 0.37670, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 5/15\n",
            "164/164 [==============================] - 330s 2s/step - loss: 0.4057 - categorical_accuracy: 0.4699 - val_loss: 0.3192 - val_categorical_accuracy: 0.5136\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.37670 to 0.31920, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 6/15\n",
            "164/164 [==============================] - 329s 2s/step - loss: 0.3398 - categorical_accuracy: 0.4902 - val_loss: 0.2904 - val_categorical_accuracy: 0.4653\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.31920 to 0.29043, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 7/15\n",
            "164/164 [==============================] - 329s 2s/step - loss: 0.2997 - categorical_accuracy: 0.5170 - val_loss: 0.2746 - val_categorical_accuracy: 0.4868\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.29043 to 0.27455, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 8/15\n",
            "164/164 [==============================] - 329s 2s/step - loss: 0.2728 - categorical_accuracy: 0.5276 - val_loss: 0.2405 - val_categorical_accuracy: 0.5502\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.27455 to 0.24053, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 9/15\n",
            "164/164 [==============================] - 328s 2s/step - loss: 0.2554 - categorical_accuracy: 0.5369 - val_loss: 0.2353 - val_categorical_accuracy: 0.5483\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.24053 to 0.23527, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 10/15\n",
            "164/164 [==============================] - 328s 2s/step - loss: 0.2460 - categorical_accuracy: 0.5397 - val_loss: 0.2463 - val_categorical_accuracy: 0.5405\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.23527\n",
            "Epoch 11/15\n",
            "164/164 [==============================] - 328s 2s/step - loss: 0.2406 - categorical_accuracy: 0.5457 - val_loss: 0.2511 - val_categorical_accuracy: 0.5315\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.23527\n",
            "Epoch 12/15\n",
            "164/164 [==============================] - 328s 2s/step - loss: 0.2338 - categorical_accuracy: 0.5557 - val_loss: 0.2475 - val_categorical_accuracy: 0.5194\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.23527\n",
            "Epoch 13/15\n",
            "164/164 [==============================] - 328s 2s/step - loss: 0.2303 - categorical_accuracy: 0.5638 - val_loss: 0.2848 - val_categorical_accuracy: 0.4242\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.23527\n",
            "Epoch 14/15\n",
            "164/164 [==============================] - 328s 2s/step - loss: 0.2230 - categorical_accuracy: 0.5726 - val_loss: 0.2234 - val_categorical_accuracy: 0.5608\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.23527 to 0.22341, saving model to ./outputs/20210313-073703/lastbest-0.hdf5\n",
            "Epoch 15/15\n",
            "164/164 [==============================] - 328s 2s/step - loss: 0.2233 - categorical_accuracy: 0.5696 - val_loss: 0.2602 - val_categorical_accuracy: 0.4991\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.22341\n",
            "Completed training after 15 epochs.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:576: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c745fe6c9706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-82707f02ec0f>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# Generate and print classification metrics and confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_NAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_NAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'classification_report.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2004\u001b[0m                                                   \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2005\u001b[0m                                                   \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2006\u001b[0;31m                                                   zero_division=zero_division)\n\u001b[0m\u001b[1;32m   2007\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     MCM = multilabel_confusion_matrix(y_true, y_pred,\n\u001b[1;32m   1489\u001b[0m                                       \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m                                       labels=labels, samplewise=samplewise)\n\u001b[0m\u001b[1;32m   1491\u001b[0m     \u001b[0mtp_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m     \u001b[0mpred_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0msorted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         return _encode_numpy(values, uniques, encode,\n\u001b[0;32m--> 118\u001b[0;31m                              check_unknown=check_unknown)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 raise ValueError(\"y contains previously unseen labels: %s\"\n\u001b[0;32m---> 50\u001b[0;31m                                  % str(diff))\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPXh8ukmJsTb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUXfNXd0JsQz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqXVLakpJsNz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fKoHivdJsJk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}